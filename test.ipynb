{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import UniSkip\n",
    "from config import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import nltk\n",
    "\n",
    "from vocab import load_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making reverse dictionary\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-4\n",
    "save_loc = \"./saved_models/skip-best\".format(lr, VOCAB_SIZE)\n",
    "mod = UniSkip()\n",
    "if USE_CUDA:\n",
    "    mod.cuda(CUDA_DEVICE)\n",
    "mod.load_state_dict(torch.load(save_loc))\n",
    "\n",
    "encoder = mod.encoder\n",
    "\n",
    "language = \"english\"\n",
    "d = DataLoader(sentences=[''], word_dict=load_dictionary('./dataset/'+language+'/corpus.txt.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lucas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gather\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def prepare_test(sentence):\n",
    "    return \" \".join(nltk.word_tokenize(sentence))\n",
    "\n",
    "def get_vector(sentence):\n",
    "    indices = d.convert_sentence_to_indices(sentence)\n",
    "    output, _ = encoder(torch.stack([indices]))\n",
    "    return output\n",
    "    \n",
    "def get_closest_sentence(sentence, target_author):\n",
    "    path = gather.get_corpus_location(language, target_author)\n",
    "    author_sentences = DataLoader(path)\n",
    "    \n",
    "    target_vector = get_vector(prepare_test(sentence))\n",
    "    \n",
    "    batch_size = 64\n",
    "    max_sim = 0\n",
    "    max_sent = []\n",
    "    n_sent = len(author_sentences.sentences)\n",
    "    \n",
    "    for i in range(0, 5000, batch_size):\n",
    "        batch = []\n",
    "        for j in range(i, min(i + batch_size, n_sent)):\n",
    "            sent = author_sentences.sentences[j]\n",
    "            ind = d.convert_sentence_to_indices(sent)\n",
    "            batch.append(ind)\n",
    "        output, _ = encoder(torch.stack(batch))\n",
    "        \n",
    "        sim = F.cosine_similarity(output, target_vector)\n",
    "        val, ind = sim.max(0)\n",
    "\n",
    "        if val > max_sim:\n",
    "            max_sim = val\n",
    "            max_sent = batch[ind]\n",
    "    return max_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Hello there, I'm happy.\"\n",
    "target_author = \"shakespeare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD: OK\n",
      "Loading text file at /home/lucas/Projets/DLDIY/dataset/english/shakespeare/t8.shakespeare.txt.std\n",
      "Making dictionary for these words\n",
      "Using cached dictionary at /home/lucas/Projets/DLDIY/dataset/english/shakespeare/t8.shakespeare.txt.std.pkl\n",
      "Making reverse dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/torch/nn/functional.py:1090: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1, 273,   2,  10, 132,  43,   3,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "res = get_closest_sentence(test_sentence, target_author)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK God, I think so.\n"
     ]
    }
   ],
   "source": [
    "sent = [x for x in res if x != 0]\n",
    "sent = d.convert_indices_to_sentences(sent)\n",
    "\n",
    "from sacremoses import MosesDetokenizer\n",
    "detokenizer = MosesDetokenizer()\n",
    "\n",
    "print(detokenizer.detokenize(sent.split(\" \"), return_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
